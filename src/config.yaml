# ===============================================================
# Configuration for L1-norm Pruning of UNet on ACDC Dataset
# ===============================================================

experiment:
  experiment_name: "exp21_new_train_summary"
  model_name: "UNet_ACDC"
  seed: 42
  device: "cuda"

# ---------------------------------------------------------------
# TRAINING CONFIGURATION
# ---------------------------------------------------------------
train:
  phase: "training"               # "training" or "retraining"
  val_ratio: 0.2
  batch_size: 8
  num_slices_per_volume: null
  model:
    in_channels: 1
    out_channels: 4
    features: [64, 128, 256, 512, 1024, 2048]
  parameters:
    learning_rate: 1e-4
    num_epochs: 10
    save_interval: 10
    loss_function: "ce"
  paths:
    train_dir: "/media/ttoxopeus/datasets/nnUNet_raw/Dataset200_ACDC/imagesTr"
    label_dir: "/media/ttoxopeus/datasets/nnUNet_raw/Dataset200_ACDC/labelsTr"
    save_root: "results"

# ---------------------------------------------------------------
# EVALUATION CONFIGURATION
# ---------------------------------------------------------------
evaluation:
  phase: "baseline_evaluation"        # "baseline_evaluation", "pruned_evaluation", "retrained_pruned_evaluation"
  batch_size: 1
  num_slices_per_volume: null
  num_visuals: 3
  paths:
    eval_dir: "/media/ttoxopeus/datasets/nnUNet_raw/Dataset200_ACDC/imagesTs"
    label_dir: "/media/ttoxopeus/datasets/nnUNet_raw/Dataset200_ACDC/labelsTs"
    save_root: "results"

# ---------------------------------------------------------------
# PRUNING CONFIGURATION
# ---------------------------------------------------------------
pruning:
  method: "l1_norm"
  ckpt_path:
    subfolder: "baseline"           # where baseline model is stored
    ckpt_name: "final_model.pth"
  save_path:
    subfolder: "pruned"             # where to save pruned results
    filename: "pruned_model.pth"
  ratios:
    default: 0.25
    block_ratios:
      encoders.0: 0.15
      encoders.1: 0.15
      encoders.2: 0.25
      encoders.3: 0.25
      encoders.4: 0.3
      encoders.5: 0.3
      bottleneck: 0.1
      decoders.1: 0.1
      decoders.3: 0.15
      decoders.5: 0.2
      decoders.7: 0.25
      decoders.9: 0.05
      decoders.11: 0.1
